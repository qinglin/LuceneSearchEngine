/*Authors: Patricia Chin, Qing Lin Xia

Some code was borrowed from http://code.google.com/p/crawler4j/ Â©Yasser Ganjisaffar
*/

package src.ir.assignments.three;

import java.io.FileWriter;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.SortedSet;
import java.util.TreeSet;

import assignment1.Frequency;
import assignment1.FrequencyComparator;

import edu.uci.ics.crawler4j.crawler.CrawlConfig;
import edu.uci.ics.crawler4j.fetcher.PageFetcher;
import edu.uci.ics.crawler4j.robotstxt.RobotstxtConfig;
import edu.uci.ics.crawler4j.robotstxt.RobotstxtServer;
import edu.uci.ics.crawler4j.crawler.CrawlController;

public class MyCrawlController {

	  /**
	 * @param args
	 * @throws Exception
	 */
	public static void main(String[] args) throws Exception {
          String crawlStorageFolder = "./data/crawl/root";
          int numberOfCrawlers = 10;
//          int maxPagesToFetch = 1000; 
          int maxDepthOfCrawling = -1;
          CrawlConfig config = new CrawlConfig();
          config.setCrawlStorageFolder(crawlStorageFolder);
          config.setMaxDepthOfCrawling(maxDepthOfCrawling);
          config.setResumableCrawling(true);
          // set the user agent id
          config.setUserAgentString("UCI IR crawler 59601138, 32882712"); 
          
          // set politeness delay 
          config.setPolitenessDelay(300);
          
          // set max number of pages to crawl 
//          config.setMaxPagesToFetch(maxPagesToFetch);

          /*
           * Instantiate the controller for this crawl.
           */
          PageFetcher pageFetcher = new PageFetcher(config);
          RobotstxtConfig robotstxtConfig = new RobotstxtConfig();
          RobotstxtServer robotstxtServer = new RobotstxtServer(robotstxtConfig, pageFetcher);
          CrawlController controller = new CrawlController(config, pageFetcher, robotstxtServer);

          /*
           * For each crawl, you need to add some seed urls. These are the first
           * URLs that are fetched and then the crawler starts following links
           * which are found in these pages
           */
          
          controller.addSeed("http://www.ics.uci.edu/");

          /*
           * Start the crawl. This is a blocking operation, meaning that your code
           * will reach the line after this only when crawling is finished.
           */
          controller.start(Crawler.class, numberOfCrawlers);  
          
          // collect statistics of the crawl and print them out
          List<Object> crawlersLocalData = controller.getCrawlersLocalData();
          long totalUniqueURLs = 0;
          long longestPage = 0;
          double totalTime = 0;
          int numOfSubdomain = 0;
          Map<String, Subdomain> subdomains = new HashMap<String, Subdomain>();
          Set<String> urls = new HashSet<String>();
          String subdomain ="";
          FileWriter fw = new FileWriter("DomainContent.txt");
          for (Object localData : crawlersLocalData) {
                  Statistic stat = (Statistic) localData;
                  Set<String> localURLs= stat.getUniqueURLs();
                  for(String url: localURLs)
                  {
                	  if(!urls.contains(url))
                		  urls.add(url);
                  }
                  totalUniqueURLs += stat.getTotalUniqueURLs();
                  if(longestPage < stat.getLongestPage())
                  {
                	  longestPage = stat.getLongestPage();
                  }
                  
                  totalTime = stat.getTotalTime();
                  Map<String, Subdomain> localSubdomains = stat.getSubdomains();          
                  for (String subKey: localSubdomains.keySet())
                  {
                	  if (!subdomains.containsKey(subKey)) 
                	  {
                		  subdomains.put(subKey, localSubdomains.get(subKey));
                	  }
                	  else
                	  {
                		  subdomains.get(subKey).incrementNumOfURL(localSubdomains.get(subKey).getNumber()); 
                	  }
                		  
                  }
                  fw.write(stat.getDomainContent());
          }
          fw.close();
  
          totalUniqueURLs = urls.size();
          numOfSubdomain = subdomains.size();
          subdomain += "Number of subdomains: " + numOfSubdomain+"\n\n"; 
          subdomain += "Subdomains: \n\n"; 
          Comparator<Subdomain> Scomparator = new SubdomainComparator();		
          SortedSet<Subdomain> keys = new TreeSet<Subdomain>(Scomparator);
          Iterator<Subdomain> it = keys.iterator();
          while(it.hasNext())
          {
        	   Subdomain next = (Subdomain)it.next();
        	   subdomain += next.getURL() + ", " + next.getNumber()+"\n\n";
          }  
          // calculate the total time (in hours, minutes, seconds, and milliseconds)
          int hours;
          int min; 
          int sec; 
          int ms; 
          hours = (int)(totalTime / 3600000); 
          totalTime -= hours * 3600000; 
          min = (int)(totalTime / 60000); 
          totalTime -= min * 60000; 
          sec = (int) (totalTime / 1000); 
          totalTime -= sec * 1000; 
          ms = (int)totalTime; 
          
          FileWriter fwStats = new FileWriter("CrawlStats.txt");  
	  	  String stats = "Total Number of Unique URLs: " + totalUniqueURLs + "\n\n" + 
	  			"Length of longest page: " + longestPage + "\n\n Total time: " + hours + 
	  			" hrs. " + min + " min. " + sec + " s. " + ms + " ms.\n\n";
	  	  fwStats.write(stats);
	  	  fwStats.write(subdomain);
	  	  fwStats.close();
  }

}
